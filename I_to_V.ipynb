{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4p1ysFKMbs_"
      },
      "source": [
        "# **IMAGE TO VIDEO WITH LTXV 0.9.7 13B DISTILLED**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBB00lC6q-DA"
      },
      "source": [
        "- You can use the free T4 GPU to generate a 5-second video (120 frames) in about 10 minutes without upscaling.\n",
        "\n",
        "- For upscaled videos, the T4 GPU can handle up to 1 second (25 frames).\n",
        "\n",
        "- For longer or faster video generation with upscale, consider using higher-tier GPUs.\n",
        "\n",
        "- All videos are generated at 24 FPS.\n",
        "\n",
        "- To generate a video with n frames, set the frames value to n + 1. To create a 5-second video (120 frames), set frames = 121.\n",
        "\n",
        "- The output video will match the resolution of the uploaded image.\n",
        "\n",
        "- Enabling the upscale_video option will generate a second, higher-quality version of the video at twice the original resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrXFIT4fMfyJ",
        "outputId": "7a004f3c-1063-41af-f396-6043c581aebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment Setup Complete!\n"
          ]
        }
      ],
      "source": [
        "# @title Prepare Environment\n",
        "# !pip install --upgrade --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "!pip install --quiet torch torchvision\n",
        "\n",
        "%cd /content\n",
        "Use_t5xxl_fp16 = False\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate\n",
        "# !pip install xformers\n",
        "!pip install av\n",
        "# !git clone --branch ComfyUI_v0.3.34 https://github.com/Isi-dev/ComfyUI\n",
        "# %cd /content/imvid/custom_nodes\n",
        "# !git clone --branch forHidream https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "# !git clone https://github.com/Isi-dev/ComfyUI_LTXVideo\n",
        "%cd /content/imvid/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_LTXVideo\n",
        "!pip install -r requirements.txt\n",
        "# %cd /content/ComfyUI\n",
        "!apt -y install -qq aria2 ffmpeg\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import os\n",
        "import imageio\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAELoader,\n",
        "    VAEDecode,\n",
        "    VAEDecodeTiled,\n",
        "    LoadImage,\n",
        "    ImageScale,\n",
        "    SaveImage\n",
        ")\n",
        "\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import (\n",
        "    UnetLoaderGGUF\n",
        ")\n",
        "\n",
        "from comfy_extras.nodes_custom_sampler import (\n",
        "    KSamplerSelect,\n",
        "    SamplerCustom,\n",
        "    RandomNoise\n",
        ")\n",
        "\n",
        "from comfy_extras.nodes_lt import (\n",
        "    LTXVPreprocess,\n",
        "    LTXVImgToVideo,\n",
        "    LTXVScheduler,\n",
        "    LTXVConditioning\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.stg import STGGuiderAdvancedNode\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.easy_samplers import LTXVBaseSampler\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.latent_upsampler import (\n",
        "    LTXVLatentUpsamplerModelLoader,\n",
        "    LTXVLatentUpsampler\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.latent_adain import LTXVAdainLatent\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.tiled_sampler import LTXVTiledSampler\n",
        "\n",
        "from custom_nodes.ComfyUI_LTXVideo.film_grain import LTXVFilmGrain\n",
        "\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/wsbagnsv1/ltxv-13b-0.9.7-distilled-GGUF/resolve/main/ltxv-13b-0.9.7-distilled-Q6_K.gguf -d /content/imvid/models/diffusion_models -o ltxv-13b-0.9.7-distilled-Q6_K.gguf\n",
        "if Use_t5xxl_fp16:\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp16.safetensors -d /content/imvid/models/text_encoders -o t5xxl_fp16.safetensors\n",
        "else:\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors -d /content/imvid/models/text_encoders -o t5xxl_fp8_e4m3fn_scaled.safetensors\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/wsbagnsv1/ltxv-13b-0.9.7-dev-GGUF/resolve/main/ltxv-13b-0.9.7-vae-BF16.safetensors -d /content/imvid/models/vae -o ltxv-13b-0.9.7-vae-BF16.safetensors\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-spatial-upscaler-0.9.7.safetensors -d /content/imvid/models/upscale_models -o ltxv-spatial-upscaler-0.9.7.safetensors\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-temporal-upscaler-0.9.7.safetensors -d /content/imvid/models/upscale_models -o ltxv-temporal-upscaler-0.9.7.safetensors\n",
        "\n",
        "clear_output()\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def upload_image():\n",
        "    \"\"\"Handle image upload in Colab and store in /content/ComfyUI/input/\"\"\"\n",
        "    from google.colab import files\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Move each uploaded file to ComfyUI input directory\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "\n",
        "        shutil.move(src_path, dest_path)\n",
        "        print(f\"Image saved to: {dest_path}\")\n",
        "        return dest_path\n",
        "\n",
        "    return None\n",
        "\n",
        "def string_to_float(string):\n",
        "        float_list = [float(x.strip()) for x in string.split(',')]\n",
        "        return (float_list,)\n",
        "\n",
        "def float_to_sigmas(float_list):\n",
        "        return torch.tensor(float_list, dtype=torch.float32),\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def generate_video(\n",
        "    image_path: str = None,\n",
        "    positive_prompt: str = \"A red fox moving gracefully\",\n",
        "    negative_prompt: str = \"low quality, worst quality\",\n",
        "    width: int = 768,\n",
        "    height: int = 512,\n",
        "    seed: int = 0,\n",
        "    steps: int = 30,\n",
        "    cfg_scale: float = 2.05,\n",
        "    sampler_name: str = \"euler\",\n",
        "    length: int = 24,  # Number of frames\n",
        "    fps: int = 24,\n",
        "    upscale_video: bool = False\n",
        "):\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        unet_loader = UnetLoaderGGUF()\n",
        "        vae_loader = VAELoader()\n",
        "        checkpoint_loader = CheckpointLoaderSimple()\n",
        "        clip_loader = CLIPLoader()\n",
        "        clip_encode_positive = CLIPTextEncode()\n",
        "        clip_encode_negative = CLIPTextEncode()\n",
        "        load_image = LoadImage()\n",
        "        image_scaler = ImageScale()\n",
        "        save_node = SaveImage()\n",
        "        preprocess = LTXVPreprocess()\n",
        "        img_to_video = LTXVImgToVideo()\n",
        "        scheduler = LTXVScheduler()\n",
        "        sampler_select = KSamplerSelect()\n",
        "        random_noise = RandomNoise()\n",
        "        conditioning = LTXVConditioning()\n",
        "        sampler = SamplerCustom()\n",
        "        vae_decode = VAEDecode()\n",
        "        stg_guider_advanced = STGGuiderAdvancedNode()\n",
        "        ltxv_base_sampler = LTXVBaseSampler()\n",
        "        vae_decode_tiled = VAEDecodeTiled()\n",
        "        upscale_model_loader = LTXVLatentUpsamplerModelLoader()\n",
        "        latent_upsampler = LTXVLatentUpsampler()\n",
        "        adain_latent = LTXVAdainLatent()\n",
        "        tiled_sampler = LTXVTiledSampler()\n",
        "        film_grain = LTXVFilmGrain()\n",
        "\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(\"t5xxl_fp8_e4m3fn_scaled.safetensors\", \"ltxv\", \"default\")[0]\n",
        "\n",
        "        assert width % 32 == 0, \"Width must be divisible by 32\"\n",
        "        assert height % 32 == 0, \"Height must be divisible by 32\"\n",
        "\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        if image_path is None:\n",
        "            print(\"Please upload an image file:\")\n",
        "            image_path = upload_image()\n",
        "        if image_path is None:\n",
        "            print(\"No image uploaded!\")\n",
        "        loaded_image = load_image.load_image(image_path)[0]\n",
        "        # processed_image = preprocess.preprocess(loaded_image, 40)[0]\n",
        "\n",
        "        width_int, height_int = image_width_height(loaded_image)\n",
        "\n",
        "        if width == 0 and height == 0 :\n",
        "            if width_int > height_int:\n",
        "                width = 768\n",
        "                height = 512\n",
        "            elif width_int == height_int:\n",
        "                width = 512\n",
        "                height = 512\n",
        "            else:\n",
        "                width = 512\n",
        "                height = 768\n",
        "\n",
        "\n",
        "        print(\"Loading UNet model...\")\n",
        "        model = unet_loader.load_unet(\"ltxv-13b-0.9.7-distilled-Q6_K.gguf\")[0]\n",
        "\n",
        "        conditionedPositive, conditionedNegative = conditioning.append(positive, negative, 25.0)\n",
        "\n",
        "        guider = stg_guider_advanced.get_guider(\n",
        "            model,\n",
        "            conditionedPositive,\n",
        "            conditionedNegative,\n",
        "            0.997,  # skip_steps_sigma_threshold\n",
        "            True,    # cfg_star_rescale\n",
        "            \"1.0, 0.9933, 0.9850, 0.9767, 0.9008, 0.6180\",  # sigmas\n",
        "            \"1,1,1,1,1,1\",  # cfg_values\n",
        "            \"0,0,0,0,0,0\",  # stg_scale_values\n",
        "            \"1, 1, 1, 1, 1, 1\",  # stg_rescale_values\n",
        "            \"[25], [35], [35], [42], [42], [42]\"  # stg_layers_indices\n",
        "        )[0]\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = vae_loader.load_vae(\"ltxv-13b-0.9.7-vae-BF16.safetensors\")[0]\n",
        "\n",
        "        # video_output = img_to_video.generate(\n",
        "        #     positive=positive,\n",
        "        #     negative=negative,\n",
        "        #     vae=vae,\n",
        "        #     image=processed_image,\n",
        "        #     width=width,\n",
        "        #     height=height,\n",
        "        #     length=length,\n",
        "        #     batch_size=1\n",
        "        # )\n",
        "\n",
        "        # sigmas = scheduler.get_sigmas(steps, cfg_scale, 0.95, True, 0.1)[0]\n",
        "        selected_sampler = sampler_select.get_sampler(sampler_name)[0]\n",
        "        # conditioned = conditioning.append(video_output[0], video_output[1], 25.0)\n",
        "\n",
        "        sigmas = float_to_sigmas(\n",
        "            string_to_float(\"1.0000, 0.9937, 0.9875, 0.9812, 0.9750, 0.9094, 0.7250, 0.4219, 0.0\")[0]\n",
        "        )[0]\n",
        "\n",
        "        noise = random_noise.get_noise(seed)[0]\n",
        "\n",
        "        loaded_image = image_scaler.upscale(\n",
        "            loaded_image,\n",
        "            \"lanczos\",\n",
        "            width,\n",
        "            height,\n",
        "            \"disabled\"\n",
        "        )[0]\n",
        "\n",
        "        try:\n",
        "\n",
        "            print(\"Generating video...\")\n",
        "\n",
        "            sampled=ltxv_base_sampler.sample(\n",
        "                model,\n",
        "                vae,\n",
        "                width,\n",
        "                height,\n",
        "                length,\n",
        "                guider,\n",
        "                selected_sampler,\n",
        "                sigmas,\n",
        "                noise,\n",
        "                optional_cond_images=loaded_image,\n",
        "                optional_cond_indices=\"0\",\n",
        "                strength=0.8,\n",
        "                crop=\"disabled\",\n",
        "                crf=30,\n",
        "                blur=1\n",
        "            )[0]\n",
        "\n",
        "            # sampled = sampler.sample(\n",
        "            #     model=model,\n",
        "            #     add_noise=True,\n",
        "            #     noise_seed=seed if seed != 0 else random.randint(0, 2**32),\n",
        "            #     cfg=cfg_scale,\n",
        "            #     positive=conditioned[0],\n",
        "            #     negative=conditioned[1],\n",
        "            #     sampler=selected_sampler,\n",
        "            #     sigmas=sigmas,\n",
        "            #     latent_image=video_output[2]\n",
        "            # )[0]\n",
        "\n",
        "            # model_management.soft_empty_cache()\n",
        "            del model\n",
        "            del guider\n",
        "            del noise\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "            try:\n",
        "                print(\"Decodimg Latents...\")\n",
        "                decoded = vae_decode.decode(vae, sampled)[0].detach()\n",
        "                # print(f\"Decoded frames shape: {decoded.shape}\")\n",
        "                # print(\"Latents Decoded!\")\n",
        "                if upscale_video is False:\n",
        "                    del vae\n",
        "                    del sampled\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during decoding: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "            # Reshape to video frames (batch, frames, H, W, C)\n",
        "            # decoded_frames = decoded.reshape(1, length, height, width, 3)\n",
        "\n",
        "            # save_node.save_images(decoded, filename_prefix=\"video_frame\")\n",
        "            decoded = image_scaler.upscale(\n",
        "                decoded,\n",
        "                \"lanczos\",\n",
        "                width_int,\n",
        "                height_int,\n",
        "                \"disabled\"\n",
        "            )[0]\n",
        "\n",
        "            output_path = \"/content/output.mp4\"\n",
        "            frames_np = (decoded.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "            del decoded\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "                for frame in frames_np:\n",
        "                    writer.append_data(frame)\n",
        "\n",
        "            print(f\"\\nBase Video generation complete! Displaying Video...\")\n",
        "            # print(f\"Saved {len(decoded)} frames to ComfyUI output directory\")\n",
        "            # print(f\"Video saved to: {output_path}\")\n",
        "            display_video(output_path)\n",
        "\n",
        "            if upscale_video:\n",
        "                model = unet_loader.load_unet(\"ltxv-13b-0.9.7-distilled-Q6_K.gguf\")[0]\n",
        "                upscale_model = upscale_model_loader.load_model(\n",
        "                    \"ltxv-spatial-upscaler-0.9.7.safetensors\", True, False\n",
        "                )[0]\n",
        "\n",
        "                tiled_guider = stg_guider_advanced.get_guider(\n",
        "                    model,\n",
        "                    conditionedPositive,\n",
        "                    conditionedNegative,\n",
        "                    0.997,  # skip_steps_sigma_threshold\n",
        "                    True,    # cfg_star_rescale\n",
        "                    \"1\",     # sigmas\n",
        "                    \"1\",     # cfg_values\n",
        "                    \"0\",     # stg_scale_values\n",
        "                    \"1\",     # stg_rescale_values\n",
        "                    \"[42]\"   # stg_layers_indices\n",
        "                )[0]\n",
        "\n",
        "                tiled_sigmas = float_to_sigmas(\n",
        "                    string_to_float(\"0.85, 0.7250, 0.6, 0.4219, 0.0\")[0]\n",
        "                )[0]\n",
        "\n",
        "                upscaled_latents = latent_upsampler.upsample_latent(\n",
        "                    sampled, upscale_model, vae\n",
        "                )[0]\n",
        "\n",
        "                adjusted_latents = adain_latent.batch_normalize(\n",
        "                    upscaled_latents, sampled, 0.25\n",
        "                )[0]\n",
        "\n",
        "                del sampled\n",
        "                del upscale_model\n",
        "                del upscaled_latents\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                tiled_noise = random_noise.get_noise(seed)[0]\n",
        "\n",
        "                loaded_image = image_scaler.upscale(\n",
        "                    loaded_image,\n",
        "                    \"lanczos\",\n",
        "                    width,\n",
        "                    height,\n",
        "                    \"disabled\"\n",
        "                )[0]\n",
        "\n",
        "                print(\"Generating high-res video...\")\n",
        "\n",
        "                tiled_output, _ = tiled_sampler.sample(\n",
        "                    model=model,\n",
        "                    vae=vae,\n",
        "                    noise=tiled_noise,\n",
        "                    sampler=selected_sampler,\n",
        "                    sigmas=tiled_sigmas,\n",
        "                    guider=tiled_guider,\n",
        "                    latents=adjusted_latents,\n",
        "                    optional_cond_images=loaded_image,\n",
        "                    horizontal_tiles=1,\n",
        "                    vertical_tiles=1,\n",
        "                    overlap=1,\n",
        "                    latents_cond_strength=0.15,\n",
        "                    boost_latent_similarity=False,\n",
        "                    crop=\"disabled\",\n",
        "                    optional_cond_indices=\"0\",\n",
        "                    images_cond_strengths=\"0.9\"\n",
        "                )\n",
        "\n",
        "                del model\n",
        "                del tiled_guider\n",
        "                del tiled_noise\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                upscaled_latents = tiled_output[\"samples\"]\n",
        "\n",
        "                latent_input = {\n",
        "                    \"samples\": upscaled_latents  # Should be shape [1,4,num_frames,H,W]\n",
        "                }\n",
        "\n",
        "                print(\"Decoding tiles...\")\n",
        "\n",
        "                decoded_frames = vae_decode_tiled.decode(\n",
        "                    vae, latent_input, width, 64, 64, 8\n",
        "                )[0]\n",
        "\n",
        "                decoded_frames = image_scaler.upscale(\n",
        "                    decoded_frames,\n",
        "                    \"lanczos\",\n",
        "                    width_int*2,\n",
        "                    height_int*2,\n",
        "                    \"disabled\"\n",
        "                )[0]\n",
        "\n",
        "                # decoded_frames = film_grain.add_film_grain(\n",
        "                #     decoded_frames, 0.01, 0.5\n",
        "                # )[0]\n",
        "\n",
        "                output_pathU = \"/content/upscaled.mp4\"\n",
        "                frames_npu = (decoded_frames.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "                del vae\n",
        "                del decoded_frames\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                with imageio.get_writer(output_pathU, fps=fps) as writer:\n",
        "                    for frame in frames_npu:\n",
        "                        writer.append_data(frame)\n",
        "\n",
        "                print(f\"\\nHigh-res Video generation complete! Displaying Video...\")\n",
        "                # print(f\"Saved {len(decoded)} frames to ComfyUI output directory\")\n",
        "                # print(f\"Video saved to: {output_path}\")\n",
        "                display_video(output_pathU)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during video generation: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_gpu_memory()\n",
        "\n",
        "\n",
        "def display_video(video_path):\n",
        "    \"\"\"Display video in Colab notebook with proper HTML5 player\"\"\"\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    mp4 = open(video_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "print(\"✅ Environment Setup Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cQFyBT2ZuIuA"
      },
      "outputs": [],
      "source": [
        "# @title Upload Image\n",
        "\n",
        "file_uploaded = upload_image()\n",
        "display_upload = False # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded))\n",
        "    else:\n",
        "        print(\"The image format cannot be displayed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869,
          "referenced_widgets": [
            "189d05658a3d429a914c7a3b0477834c",
            "33464cf6d478447da04255fa3ce8358f",
            "26ef4d9ade7a4d2986fc2fd25405327a",
            "edb3ffd02fcc4569bb3a31d05d08198b",
            "aa66b6842fb649f18d27a891a38a6917",
            "5446fd03aac84a0ba02c1af06e0b5ae7",
            "2d2dfcaa1b974feea0410aa157f31fff",
            "d0d78cec43154c4281945d2f00ef125a",
            "3076325205664fd08db52c30f608db3c",
            "09f30929cebf41519d4ae8c259cb33ff",
            "dc255ca0e6d9419cb3b62ac9b957d4ac"
          ]
        },
        "id": "roC59_oNNflb",
        "outputId": "bc93a5ff-ea51-4d09-8010-ac2330cdb89c"
      },
      "outputs": [],
      "source": [
        "# @title Run Image to Video\n",
        "positive_prompt = \"A beautiful woman\" # @param {\"type\":\"string\"}\n",
        "negative_prompt = \"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\" # @param {\"type\":\"string\"}\n",
        "# width = 512 # @param {\"type\":\"number\"}\n",
        "# height = 768 # @param {\"type\":\"number\"}\n",
        "width = 320  # @param {\"type\":\"number\"}\n",
        "height = 480 # @param {\"type\":\"number\"}\n",
        "seed = 0 # @param {\"type\":\"integer\"}\n",
        "steps = 20\n",
        "steps = 30 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n",
        "cfg_scale = 2.5\n",
        "sampler_name=\"euler_ancestral\" # @param [\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"]\n",
        "frames = 50 # @param {\"type\":\"integer\", \"min\":1, \"max\":120}\n",
        "upscale_video = False # @param {type:\"boolean\"}\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "\n",
        "# @title Run Video Generation\n",
        "generate_video(\n",
        "        image_path=file_uploaded,\n",
        "        positive_prompt=positive_prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        seed=seed,\n",
        "        steps=steps,\n",
        "        cfg_scale=cfg_scale,\n",
        "        sampler_name=sampler_name,\n",
        "        length=frames,\n",
        "        upscale_video=upscale_video\n",
        ")\n",
        "clear_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAstC2nodUS7"
      },
      "outputs": [],
      "source": [
        "!cp -r ../drive/MyDrive/ComfyUI /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve6J2jpVeWyU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09f30929cebf41519d4ae8c259cb33ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "189d05658a3d429a914c7a3b0477834c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33464cf6d478447da04255fa3ce8358f",
              "IPY_MODEL_26ef4d9ade7a4d2986fc2fd25405327a",
              "IPY_MODEL_edb3ffd02fcc4569bb3a31d05d08198b"
            ],
            "layout": "IPY_MODEL_aa66b6842fb649f18d27a891a38a6917"
          }
        },
        "26ef4d9ade7a4d2986fc2fd25405327a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0d78cec43154c4281945d2f00ef125a",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3076325205664fd08db52c30f608db3c",
            "value": 8
          }
        },
        "2d2dfcaa1b974feea0410aa157f31fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3076325205664fd08db52c30f608db3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33464cf6d478447da04255fa3ce8358f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5446fd03aac84a0ba02c1af06e0b5ae7",
            "placeholder": "​",
            "style": "IPY_MODEL_2d2dfcaa1b974feea0410aa157f31fff",
            "value": "100%"
          }
        },
        "5446fd03aac84a0ba02c1af06e0b5ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa66b6842fb649f18d27a891a38a6917": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d78cec43154c4281945d2f00ef125a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc255ca0e6d9419cb3b62ac9b957d4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edb3ffd02fcc4569bb3a31d05d08198b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09f30929cebf41519d4ae8c259cb33ff",
            "placeholder": "​",
            "style": "IPY_MODEL_dc255ca0e6d9419cb3b62ac9b957d4ac",
            "value": " 8/8 [01:08&lt;00:00,  9.39s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
